---
title: "Mini Data-Analysis Deliverable 1"
date: "October 8, 2021"
output: 
   github_document:
      toc: true
      toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## 1. Introduction

This mini-project is part of the deliverables in the STAT545A course. It involves installation of the `datateachr` package by Hayley Boyce and Jordan Bourak which currently consists of 7 semi-tidy datasets.

It also requires loading the `datateachr` and `tidyverse` packages which are necessary for a successful completion of the project.


**There are 3 major tasks in this project**:

+ Choose your favorite dataset
+ Explore your dataset
+ Write your research questions


### 1.1 Learning Objectives

By the end of this project, I should:

+ Become familiar with the datasets, especially the one I choose to work with in greater depth
+ Be able to formulate 4 research questions that I would like to answer with my data
+ Generate a well written and reproducible report using R Markdown


### 1.2 Install the [`datateachr`](https://github.com/UBC-MDS/datateachr) package 

To install the **datateachr** package, first install the **devtools** package using **install.packages()** with parentheses surrounding *devtools* in your **R console** and load the **devtools** package using **library()** before installing *datateachr* with this code-  **devtools::install_github("UBC-MDS/datateachr")**. It is a one-off exercise in that once it is installed, you only need to reload it whenever you start a new session to use the package and the datasets it contains.


### 1.3 Load required packages

```{r}
library(datateachr)
library(tidyverse)
```



## 2. Define variables

It is always beneficial to define the variables/datasets you will be using in your analysis early in your project. The datasets can be explored without defining them once the package containing them is loaded. However, for clarity and to notice at a glance the source of the datasets, I like to define them early enough.

```{r}
# define variables
apt_buildings <- datateachr::apt_buildings
building_permits <- datateachr::building_permits
cancer_sample <- datateachr::cancer_sample
flow_sample <- datateachr::flow_sample
parking_meters <- datateachr::parking_meters
steam_games <- datateachr::steam_games
vancouver_trees <- datateachr::vancouver_trees
```



## 3. Task 1: Choose your favorite dataset

As mentioned earlier, the following are the 7 datasets to choose from:

+ *apt_buildings*: Acquired courtesy of The City of Toronto's Open Data Portal. It currently has 3455 rows and 37 columns.
+ *building_permits*: Acquired courtesy of the City of Vancouver's Open Data Portal. It currently has 20680 rows and 14 columns.
+ *cancer_sample*: Acquired courtesy of UCI Machine Learning Repository. It currently has 569 rows and 32 columns.
+ *flow_sample*: Acquired courtesy of The Government of Canada's Historical Hydrometric Database. It currently has 218 rows and 7 columns.
+ *parking-meters*: Acquired courtesy of The City of Vancouver's Open Data Portal. It currently has 10032 rows and 22 columns.
+ *steam_games*: Acquired courtesy of Kaggle. It currently has 40833 rows and 21 columns.
+ *vancouver_trees*: Acquired courtesy of The City of Vancouver's Open Data Portal. It currently has 146611 rows and 20 columns.


**The above description is an excerpt from the milestone document for the project prepared by Icíar Fernández Boyano and launched by Vincenzo Coia**.



### 3.1. Instructions for the project as given by the teaching team

**Instruction 1.1**: Out of the 7 datasets available in the **datateachr** package, choose **4** that appeal to you based on their description.


#### 3.1.1. The 4 choices

1. apt_buildings
2. cancer_sample
3. flow_sample
4. vancouver_trees


**Instruction 1.2**: One way to narrow down your selection is to *explore* the datasets. Use your knowledge of dplyr to find out at least 3 attributes about each of these datasets. The goal is to have an idea of *what the data looks like*


#### 3.1.2. Exploration of the datasets to have an idea of what they look like

To explore each of the four chosen datasets, I will look at class of the data, variable types, number of rows and columns, the variable names, the total number of missing values (NAs). 

**The code below are from what I learned in STAT545A and the R for Data Science book by Hadley Wickham & Garrett Grolemund.**



```{r}
# explore the apt_buildings dataset
# get class of the dataset
class(apt_buildings)

# find out number of rows and columns, names of variables, variable types
glimpse(apt_buildings)

# get total number of missing values(NAs)
sum(is.na(apt_buildings))

```


From the analysis above, one can conclude that the *apt_buildings* is a dataframe that has been converted to a tibble, and has 3,455 rows and 37 columns. The data contains mostly character variables with few doubles. A total of 6286 values are missing (NAs). 




```{r}
# explore the cancer_sample dataset
# get class of the dataset
class(cancer_sample)

# find out number of rows and columns, names of variables, variable types
glimpse(cancer_sample)

# get total number of missing values(NAs)
sum(is.na(cancer_sample))
```


The analysis above shows that the *cancer_sample* dataset is a dataframe that has been converted to a tibble, and has 569 rows and 32 columns. Most of the variables are doubles with only 1 character variable. Surprisingly, there's no missing value in the dataset.




```{r}
# explore the flow_sample dataset
# get class of the dataset
class(flow_sample)

# determine number of rows and columns, names of variables, variable types
glimpse(flow_sample)

# get total number of missing values(NAs)
sum(is.na(flow_sample))
```


The *flow_sample* dataset is a dataframe that has been converted to a tibble, and has 218 rows and 7 columns. The variable types are roughly equal- 3 characters and 4 doubles. There are 125 missing values in the dataset.




```{r}
# explore the vancouver_trees dataset
#  get class of the dataset
class(vancouver_trees)

# determine number of rows and columns, names of variables, variable types
glimpse(vancouver_trees)

# get total number of missing values
sum(is.na(vancouver_trees))
```


To summarize findings from the *vancouver_trees* dataset, it is a dataframe that has been converted to a tibble, and comprises 146,611 rows and 20 columns. There are more character variables than doubles. There's one date variable type. There are multiple missing values in the dataset(191,135).




**Instruction 1.3**: Now that you've explored the 4 datasets that you were initially interested in, let's narrow it down to 2. What led you to choose these 2? Briefly explain your choices below, and feel free to include any code in your explanation.


#### 3.1.3. The 2 choices

1. apt_buildings
2. cancer_sample



#### 3.1.4. Brief reasons for the 2 choices

**First**, I chose **apt_buildings** because when I explored it, I could see that there are `r ncol(apt_buildings)` columns in the dataset with `year_built` and `no_of_storeys` as part of the variables. So, I became curious to see if the number of storeys built changed as the years rolled by. Therefore, I ran the code below.


```{r}
# Explore the relationship between the year a building was built and the number of storeys built in a plot
apt_buildings %>% ggplot(aes(year_built, no_of_storeys)) +
  geom_jitter(alpha = 0.1, width = 0.2, na.rm = TRUE) + theme_minimal()
```



**Second**, I chose **cancer_sample** because I wanted to investigate the difference in concavity of nuclei based on diagnosis (benign or malignant). To find out, I ran the code below.

```{r}
# visualize the mean concavity of nuclei of samples based on diagnosis
cancer_sample %>% ggplot(aes(diagnosis, concavity_mean)) + 
  geom_boxplot() + 
  theme_minimal()
```



**Instruction 1.4**: Try to think of 1 research question that you would want to answer with each dataset. Note them down below, and make your final choice based on what seems more interesting to you!


#### 3.1.5. One research question for each dataset

+ `apt_buildings`: Did the number of storeys built change over the years?
+ `flow_sample`: Which months had the highest flow intensity?
+ `cancer_sample`: Is there a relationship between diagnosis and mean perimeter of nuclei?
+ `vancouver_trees`: Does each tree planted have a root barrier installed?



**Final choice**: Out of the four research questions, it seems investigating whether there's a relationship between cancer diagnosis (benign or malignant) and mean perimeter of sample nuclei would be the most interesting.Therefore, I choose the `cancer_sample` dataset.



## 4. Task 2: Exploring your dataset

**Instruction 2.1**: Complete *4* out of the following *8* exercises to dive deeper into your data. All datasets are different and therefore, not all of these tasks may make sense for your data - which is why you should only answer 4. Use *dplyr* and *ggplot*.

1. Plot the distribution of a numeric variable.
2. Create a new variable based on other variables in your data (only if it makes sense).
3. Investigate how many missing values there are per variable. Can you find a way to plot this?
4. Explore the relationship between 2 variables in a plot.
5. Filter observations in your data according to your own criteria. Think of what you'd like to explore - again, if this was the titanic dataset, I may want to narrow my search to passengers born in a particular year...
6. Use a boxplot to look at the frequency of different observations within a single variable. You can do this for more than one variable if you wish!
7. Make a new tibble with  a subset of your data, with variables and observations that you are interested in exploring.
8. Use a density plot to explore any of your variables (that are suitable for this type of plot).



### 4.1. The 4 exercises

```{r}
# plot the distribution of a numeric variable
cancer_sample %>% ggplot(aes(radius_mean)) + 
  geom_histogram(binwidth = 1)
```

`radius_mean` is a numeric variable in the *cancer_sample* dataset. Therefore, I wanted to investigate the spread of the mean radius across all observations. `geom_histogram` is one of the geometric objects for visualizing distribution of numeric/continuous variables in a dataset. I piped (%>%) the data into the ggplot function and mapped `radius_mean` to the x-axis as geom_histogram takes only the variable for the x-axis and computes `count` on the y-axis.



```{r}
# explore the relationship between 2 variables (diagnosis and area_mean) in a plot
cancer_sample %>% ggplot( aes(diagnosis, area_mean)) +
   geom_boxplot()
```

I wanted to look at how `area_mean`( a numeric variable) varies between benign and malignant samples under diagnosis`(a categorical variable), so I chose a boxplot which shows the 25th, 50th and 75th percentiles of the numeric variable of interest on the box and outliers on the whiskers.



```{r}
# make a new tibble with a subset of your data, with variables and observations that you are interested in exploring
cancer_sample_sub <- cancer_sample %>% select(diagnosis:area_se, -fractal_dimension_mean)
```

I used the `select` function from the `dplyr` package to subset my data by selecting all variables from *diagnosis to area_se*, using a `minus` sign to exclude *fractal_dimension_mean* which occurred within the specified sequence. All other variables outside the sequence were also dropped. I stored the new tibble in a variable named `cancer-sample_sub`.



```{r}
# use a density plot to explore any of your variables
cancer_sample %>% ggplot(aes(radius_mean)) +
  geom_density(aes(fill = diagnosis), alpha = 0.2) +
  theme_minimal()
```

A `density` plot takes a numeric variable. So, I used it to look at the distribution of the `radius_mean` of cancer nuclei between `benign` and `malignant` samples. I mapped `diagnosis` to the `fill` aesthetics and used an `alpha` of 0.2 to tone down the colour on the plot. I also added another layer- `theme_minimal`to lighten the grids in the background.



## 5. Task 3: Write your research questions

Figure out 4 research questions that you would like to answer with your data.



### 5.1. The 4 research questions I would like to answer using the *cancer_sample* data

1. Is there a relationship between diagnosis and mean perimeter of nuclei of samples?
2. Do the concave_points change as the perimeter of nuclei change?
3. Do the nuclei become more thickened as the diagnosis worsens?
4. Is there likely to be more standard error of perimeter of nuclei in malignant than benign samples?


